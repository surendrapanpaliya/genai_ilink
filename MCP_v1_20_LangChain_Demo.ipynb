{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1939ddfb",
   "metadata": {},
   "source": [
    "# üß† MCP‚ÄØ1.20‚ÄØDemo‚ÄØ+‚ÄØLangChain‚ÄØIntegration\n",
    "**Date:** October 31, 2025\n",
    "\n",
    "This notebook demonstrates a working Model‚ÄØContext‚ÄØProtocol‚ÄØ(MCP)‚ÄØServer‚ÄØ+‚ÄØClient using the Anthropic‚ÄØSDK‚ÄØ(v1.20‚ÄØseries), and an optional LangChain‚ÄØintegration with‚ÄØGPT‚Äë5.\n",
    "\n",
    "Ensure you have‚ÄØ`mcp`,‚ÄØ`langchain‚Äëmcp`,‚ÄØand‚ÄØ`langchain‚Äëopenai`‚ÄØinstalled:\n",
    "```bash\n",
    "pip install -U mcp langchain-mcp langchain-openai\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914616e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U langchain-mcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db191823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: langchain-mcp\n",
      "Version: 0.2.1\n",
      "Summary: Model Context Protocol tool calling support for LangChain\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Andrew Wason <rectalogic@rectalogic.com>\n",
      "License: \n",
      "Location: /opt/anaconda3/lib/python3.11/site-packages\n",
      "Requires: langchain-core, mcp, pydantic, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip show langchain-mcp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda6dd66",
   "metadata": {},
   "source": [
    "## üß©‚ÄØCell‚ÄØ1‚ÄØ‚Äî‚ÄØDefine‚ÄØand‚ÄØRun‚ÄØMCP‚ÄØServer\n",
    "Defines two tools:‚ÄØ`add`‚ÄØand‚ÄØ`message_of_the_day`.\n",
    "\n",
    "**Run this cell once**; it will wait for a client connection (stdio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8e453fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Model Context Protocol (MCP) is a standard way to let AI systems securely ‚Äúplug into‚Äù enterprise data and tools at runtime, without copying sensitive banking data into the model or retraining it. MCP exposes approved tools and read-only resources through a governed interface, so an AI assistant can, for example, fetch a customer profile, run a risk check, or query a ledger with least-privilege access and full audit trails. Because the protocol is model- and vendor-agnostic, banks can reuse the same controls across different AI providers and deployment modes (cloud or on‚Äëprem). Policies, permissions, and human approvals can be enforced centrally, and access can be revoked instantly. The result is faster AI integration with core systems while supporting compliance, data minimization, and operational risk controls.\n"
     ]
    }
   ],
   "source": [
    "# Basic GPT‚Äë5 text generation using the Responses API\n",
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "resp = client.responses.create(\n",
    "    model=\"gpt-5\" ,  # or the exact variant you have access to, e.g., \"gpt-5-large\"\n",
    "    input=\"Explain the Model Context Protocol in one short paragraph for a banking audience.\"\n",
    ")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4b99dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server.py\n",
    "\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"DemoServer\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two integers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def message_of_the_day() -> str:\n",
    "    \"\"\"Return a motivational message.\"\"\"\n",
    "    return \"Stay curious. Build boldly.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # FastMCP.run() is synchronous in v1.20+\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ddc81f",
   "metadata": {},
   "source": [
    "## üß™‚ÄØCell‚ÄØ2‚ÄØ‚Äî‚ÄØConnect‚ÄØClient‚ÄØand‚ÄØParse‚ÄØResults\n",
    "Launches the server automatically via stdio, lists available tools, and calls them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0185516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_client_demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_client_demo.py\n",
    "\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "async def main():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server.py\"])\n",
    "\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            tools = await session.list_tools()\n",
    "            tool_names = [t[0] for t in tools]\n",
    "            print(\"Available tools:\", tool_names)\n",
    "\n",
    "            result = await session.call_tool(\"add\", {\"a\": 7, \"b\": 8})\n",
    "            print(\"add(7,8) =\", result.structuredContent.get(\"result\"))\n",
    "\n",
    "            motd = await session.call_tool(\"message_of_the_day\", {})\n",
    "            print(\"message_of_the_day() =\", motd.structuredContent.get(\"result\"))\n",
    "\n",
    "asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "921b597f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 20:41:34]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=592778;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=203149;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "Available tools: ['meta', 'nextCursor', 'tools']\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=643674;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=785493;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "add(7,8) = 15\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=921498;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=633470;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "message_of_the_day() = Stay curious. Build boldly.\n"
     ]
    }
   ],
   "source": [
    "!python mcp_client_demo.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d8076",
   "metadata": {},
   "source": [
    "## ü§ñ‚ÄØCell‚ÄØ3‚ÄØ‚Äî‚ÄØLangChain‚ÄØ+‚ÄØGPT‚Äë5‚ÄØIntegration\n",
    "Demonstrates using‚ÄØLangChain‚ÄØto allow‚ÄØGPT‚Äë5‚ÄØto call MCP tools automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf045826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You‚Äôre connected‚ÄîI‚Äôm here and ready. How can I help today?\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "resp = client.responses.create(model=\"gpt-5\", input=\"Hello GPT-5, just testing connectivity.\")\n",
    "print(resp.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "31fc13c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing langchain_mcp_integration1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langchain_mcp_integration1.py\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp import MCPToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent  # modern agent type\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Optional: set your key here for testing\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key\"\n",
    "\n",
    "async def run_agent():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server.py\"])\n",
    "\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "\n",
    "            # Build and initialize MCP toolkit\n",
    "            toolkit = MCPToolkit(session=session)\n",
    "            await toolkit.initialize()\n",
    "            tools = toolkit.get_tools()\n",
    "            print(\"‚úÖ Loaded tools:\", [t.name for t in tools])\n",
    "\n",
    "            # Create GPT-5 LLM with timeout and verbosity\n",
    "            llm = ChatOpenAI(model=\"gpt-5\", temperature=0, request_timeout=30, verbose=True)\n",
    "\n",
    "            # Create modern LangGraph agent that supports multi-input tools\n",
    "            agent = create_react_agent(llm, tools)\n",
    "\n",
    "            print(\"ü§ñ Running agent query ...\\n\")\n",
    "            try:\n",
    "                # Use async .ainvoke() (non-blocking)\n",
    "                result = await agent.ainvoke(\n",
    "                    {\"messages\": [(\"user\", \"Add 25 and 30 using the MCP tool, then show the message of the day.\")]}\n",
    "                )\n",
    "\n",
    "                # Extract last message content safely\n",
    "                last_message = result[\"messages\"][-1].content\n",
    "                print(\"\\nüí¨ Agent Response:\\n\", last_message)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Agent execution failed: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_agent())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eb5d96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:40:39]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=907481;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=750516;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "‚úÖ Loaded tools: ['add', 'message_of_the_day']\n",
      "ü§ñ Running agent query ...\n",
      "\n",
      "\u001b[2;36m[10/31/25 22:40:45]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=688997;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=576236;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=174636;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=696049;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\n",
      "üí¨ Agent Response:\n",
      " Result: 55\n",
      "Message of the Day: Stay curious. Build boldly.\n"
     ]
    }
   ],
   "source": [
    "!python langchain_mcp_integration1.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
