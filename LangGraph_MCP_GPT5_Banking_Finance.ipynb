{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b1e1d759",
   "metadata": {},
   "source": [
    "# 🧠 LangGraph + MCP + GPT-5 for Banking and Finance\n",
    "**Date:** October 31, 2025\n",
    "\n",
    "This notebook demonstrates integration of **Model Context Protocol (MCP)** with **LangGraph** and **GPT‑5** to build tool‑calling AI workflows for **Banking & Finance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97269f98",
   "metadata": {},
   "source": [
    "## 🧩 What is Model Context Protocol (MCP)\n",
    "**MCP** is an open standard that defines how Large Language Models communicate with external tools and APIs.\n",
    "\n",
    "- Discover available tools (`list_tools`)\n",
    "- Call tools dynamically (`call_tool`)\n",
    "- Receive structured JSON responses\n",
    "\n",
    "Think of MCP as the **USB protocol for AI tools** — plug & play interoperability for any LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca09284c",
   "metadata": {},
   "source": [
    "## 🤔 Why MCP ?\n",
    "| Traditional Integration | With MCP |\n",
    "|--------------------------|----------|\n",
    "| Hard-coded APIs | Dynamic tool discovery |\n",
    "| App-specific contracts | Standardized JSON-RPC |\n",
    "| Tight coupling | Sandboxed isolation |\n",
    "| One LLM per API | Multi-LLM interoperability |\n",
    "\n",
    "**Benefits**\n",
    "- 🔌 Interoperable\n",
    "- 🔒 Secure and sandboxed\n",
    "- 🧠 Auto-discovery of tools\n",
    "- ⚙️ Works with LangGraph, Copilot, Claude"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7b1d4",
   "metadata": {},
   "source": [
    "## ⚙️ MCP Architecture\n",
    "```\n",
    "GPT‑5 ↔ LangGraph ↔ MCP Client ↔ MCP Server ↔ Business Tools\n",
    "```\n",
    "**Server** → hosts logic (e.g., EMI calculator)\n",
    "\n",
    "**Client** → bridges LangGraph ↔ Server\n",
    "\n",
    "**LLM Agent** → GPT‑5 decides which tool to call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cddec3",
   "metadata": {},
   "source": [
    "## 🛠️ Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee6ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U mcp langchain-mcp langchain-openai langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71709867",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-your-key-here\"  # Replace with valid key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8097a5a7",
   "metadata": {},
   "source": [
    "## 🧱 Basic MCP Server Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36cccdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server.py\n",
    "\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "mcp = FastMCP(\"BankingMCP\")\n",
    "\n",
    "@mcp.tool()\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@mcp.tool()\n",
    "def message_of_the_day() -> str:\n",
    "    return \"Stay curious. Build boldly.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3176f93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mcp_server.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51de70be",
   "metadata": {},
   "source": [
    "## 🧪 MCP Client Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fea42b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_client_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_client_test.py\n",
    "\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "async def test_client():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server.py\"])\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            tools = await session.list_tools()\n",
    "            print(\"Available tools:\", [t[0] for t in tools])\n",
    "            result = await session.call_tool(\"add\", {\"a\": 10, \"b\": 20})\n",
    "            print(\"Result:\", result.structuredContent.get(\"result\"))\n",
    "\n",
    "asyncio.run(test_client())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16125b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:25:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=985938;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=300098;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "Available tools: ['meta', 'nextCursor', 'tools']\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=955175;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=81691;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "Result: 30\n"
     ]
    }
   ],
   "source": [
    "!python mcp_client_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f144f283",
   "metadata": {},
   "source": [
    "## 🤖 LangGraph + GPT‑5 Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f44ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting langchain_mcp_integration.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile langchain_mcp_integration.py\n",
    "\n",
    "import asyncio, warnings\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp import MCPToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "async def run_agent():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server.py\"])\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            toolkit = MCPToolkit(session=session)\n",
    "            await toolkit.initialize()\n",
    "            tools = toolkit.get_tools()\n",
    "            print(\"✅ Loaded tools:\", [t.name for t in tools])\n",
    "\n",
    "            llm = ChatOpenAI(model=\"gpt-5\", temperature=0, request_timeout=30)\n",
    "            agent = create_react_agent(llm, tools)\n",
    "            result = await agent.ainvoke({\n",
    "                \"messages\":[(\"user\",\"Add 40 and 60 then give message of the day.\")]\n",
    "            })\n",
    "            print(\"💬 Response:\", result[\"messages\"][-1].content)\n",
    "\n",
    "asyncio.run(run_agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "949c98e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:28:07]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=643788;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=884471;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "✅ Loaded tools: ['add', 'message_of_the_day']\n",
      "\u001b[2;36m[10/31/25 22:28:17]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=8500;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=875747;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=699554;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=376926;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "💬 Response: Sum: 100\n",
      "Message of the day: Stay curious. Build boldly.\n"
     ]
    }
   ],
   "source": [
    "!python langchain_mcp_integration.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2113273",
   "metadata": {},
   "source": [
    "## 💰 Demo 1 — Loan EMI Calculator (MCP Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fe4b4b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_server_emi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server_emi.py\n",
    "import math\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "mcp = FastMCP(\"LoanEMIServer\")\n",
    "\n",
    "@mcp.tool()\n",
    "def calculate_emi(principal: float, rate: float, tenure_years: float) -> float:\n",
    "    \"\"\"Calculate monthly EMI.\"\"\"\n",
    "    r = rate / (12 * 100)\n",
    "    n = tenure_years * 12\n",
    "    emi = principal * r * ((1 + r)**n) / ((1 + r)**n - 1)\n",
    "    return round(emi, 2)\n",
    "\n",
    "@mcp.tool()\n",
    "def message_of_the_day() -> str:\n",
    "    return \"Dream big — finance smartly 💡\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d163336",
   "metadata": {},
   "source": [
    "### 🧮 EMI Client + LangGraph Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "983fced0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_client_emi.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_client_emi.py\n",
    "\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp import MCPToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "async def run_emi_agent():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server_emi.py\"])\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            toolkit = MCPToolkit(session=session)\n",
    "            await toolkit.initialize()\n",
    "            tools = toolkit.get_tools()\n",
    "            print(\"Tools:\", [t.name for t in tools])\n",
    "\n",
    "            llm = ChatOpenAI(model=\"gpt-5\", temperature=0)\n",
    "            agent = create_react_agent(llm, tools)\n",
    "            result = await agent.ainvoke({\n",
    "                \"messages\":[(\"user\",\"Calculate EMI for ₹5 L loan @ 10% for 3 years and share message of the day.\")]\n",
    "            })\n",
    "            print(\"Agent Response:\", result[\"messages\"][-1].content)\n",
    "\n",
    "asyncio.run(run_emi_agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "665a61de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:30:27]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=119933;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=970310;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "Tools: ['calculate_emi', 'message_of_the_day']\n",
      "\u001b[2;36m[10/31/25 22:30:33]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=453181;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=840959;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=98476;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=578;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "Agent Response: Here you go:\n",
      "\n",
      "- EMI for ₹5,00,000 at 10% for 3 years: ₹16,133.59 per month\n",
      "- Message of the Day: Dream big — finance smartly 💡\n"
     ]
    }
   ],
   "source": [
    "!python mcp_client_emi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a79ceab",
   "metadata": {},
   "source": [
    "## 🧮 Demo 2 — Risk Scoring Tool (MCP Server)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e756a202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_server_risk.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_server_risk.py\n",
    "\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "mcp = FastMCP(\"RiskServer\")\n",
    "\n",
    "@mcp.tool()\n",
    "def calculate_risk_score(age: int, income: float, liabilities: float) -> str:\n",
    "    \"\"\"Return basic financial risk score.\"\"\"\n",
    "    ratio = liabilities / income if income else 1\n",
    "    score = max(0, 100 - (age*0.2 + ratio*50))\n",
    "    if score > 75: return \"Low Risk\"\n",
    "    elif score > 40: return \"Moderate Risk\"\n",
    "    else: return \"High Risk\"\n",
    "\n",
    "@mcp.tool()\n",
    "def message_of_the_day() -> str:\n",
    "    return \"Plan your finances before they plan you 💼\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691e9743",
   "metadata": {},
   "source": [
    "### 🧠 Risk Scoring Client + LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1896549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mcp_client_risk.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_client_risk.py\n",
    "\n",
    "import asyncio\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "from langchain_mcp import MCPToolkit\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "async def run_risk_agent():\n",
    "    params = StdioServerParameters(command=\"python\", args=[\"mcp_server_risk.py\"])\n",
    "    async with stdio_client(params) as (r, w):\n",
    "        async with ClientSession(r, w) as session:\n",
    "            await session.initialize()\n",
    "            toolkit = MCPToolkit(session=session)\n",
    "            await toolkit.initialize()\n",
    "            tools = toolkit.get_tools()\n",
    "            print(\"Tools:\", [t.name for t in tools])\n",
    "\n",
    "            llm = ChatOpenAI(model=\"gpt-5\", temperature=0)\n",
    "            agent = create_react_agent(llm, tools)\n",
    "            result = await agent.ainvoke({\n",
    "                \"messages\":[(\"user\",\"Calculate risk score for age = 35, income = 15 L INR, liabilities = 3 L INR.\")]\n",
    "            })\n",
    "            print(\"Agent Response:\", result[\"messages\"][-1].content)\n",
    "\n",
    "asyncio.run(run_risk_agent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f89c943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2;36m[10/31/25 22:32:06]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=139469;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=63451;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         ListToolsRequest                      \u001b[2m             \u001b[0m\n",
      "Tools: ['calculate_risk_score', 'message_of_the_day']\n",
      "\u001b[2;36m[10/31/25 22:32:11]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m Processing request of type            \u001b]8;id=300616;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py\u001b\\\u001b[2mserver.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=487186;file:///opt/anaconda3/lib/python3.11/site-packages/mcp/server/lowlevel/server.py#674\u001b\\\u001b[2m674\u001b[0m\u001b]8;;\u001b\\\n",
      "\u001b[2;36m                    \u001b[0m         CallToolRequest                       \u001b[2m             \u001b[0m\n",
      "Agent Response: Your financial risk score is: Low Risk.\n",
      "\n",
      "Inputs considered:\n",
      "- Age: 35\n",
      "- Income: ₹15,00,000 per year\n",
      "- Liabilities: ₹3,00,000\n",
      "\n",
      "If you’d like, I can break down how this score is calculated or help you see how changes (e.g., higher liabilities or lower income) would affect it.\n"
     ]
    }
   ],
   "source": [
    "!python mcp_client_risk.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e8cc5f",
   "metadata": {},
   "source": [
    "## 🔧 Future Demos (Placeholders)\n",
    "- Account Balance Calculator\n",
    "- Credit Card Reward Estimator\n",
    "- Transaction Analyzer (Alerts)\n",
    "- Natural Language Banking Query Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da60800",
   "metadata": {},
   "source": [
    "## ✅ Summary\n",
    "- MCP standardizes tool calling between LLMs and code.\n",
    "- LangGraph adds stateful multi‑tool reasoning.\n",
    "- GPT‑5 acts as the controller to invoke banking tools intelligently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829e26c5",
   "metadata": {},
   "source": [
    "## 📚 References\n",
    "- [ModelContextProtocol.io](https://modelcontextprotocol.io)\n",
    "- [LangGraph Docs](https://langchain-ai.github.io/langgraph/)\n",
    "- [LangChain MCP GitHub](https://github.com/rectalogic/langchain-mcp)\n",
    "- [OpenAI GPT‑5 Docs](https://platform.openai.com/docs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
