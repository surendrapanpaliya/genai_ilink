{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c56b48e5",
   "metadata": {},
   "source": [
    "\n",
    "## LangGraph + LangChain\n",
    "\n",
    "This hands-on notebook introduces **LangGraph** and **LangChain (OpenAI)** for agentic AI in Banking/Finance:\n",
    "- **Intro to LangGraph**\n",
    "- **LangGraph basic example (typed state)**  \n",
    "- **LangGraph with GPT (via `langchain-openai`)** *(model name configurable; use `gpt-5` if your account has access)*\n",
    "- **Use case: LangChain + LangGraph + GPT â€” banking FAQ summarizer**\n",
    "- **Multi-Agent Orchestration:** coordinating agents, context passing, decision routing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507094b",
   "metadata": {},
   "source": [
    "## LangGraph \n",
    "\n",
    "LangGraph is very low-level, and focused entirely on agent orchestration. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with models and tools."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e72822",
   "metadata": {},
   "source": [
    "\n",
    "## 0) Setup & Versions\n",
    "Install/upgrade required packages. If your org pins versions, adapt accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "05eeaaf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.8 | packaged by conda-forge | (main, Feb 16 2024, 20:49:36) [Clang 16.0.6 ]\n",
      "Platform: macOS-26.0.1-arm64-arm-64bit\n",
      "Loaded .env\n",
      "OPENAI_API_KEY set: True\n",
      "OPENAI_MODEL: gpt-5\n"
     ]
    }
   ],
   "source": [
    "# If needed, uncomment to install/upgrade.\n",
    "# %pip install -U langgraph langchain langchain-core langchain-openai python-dotenv\n",
    "\n",
    "import sys, platform, os\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "\n",
    "# Optional: load a .env if present for OPENAI_API_KEY and OPENAI_MODEL\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv()\n",
    "    print(\"Loaded .env\")\n",
    "except Exception as e:\n",
    "    print(\"python-dotenv not installed or .env absent (ok).\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5\")  # set to \"gpt-5\" if your account has access\n",
    "print(\"OPENAI_API_KEY set:\", bool(OPENAI_API_KEY))\n",
    "print(\"OPENAI_MODEL:\", OPENAI_MODEL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb3f4d4",
   "metadata": {},
   "source": [
    "\n",
    "## 1) LangGraph â€” What & Why (Quick Intro)\n",
    "\n",
    "**LangGraph** helps you build **agentic workflows** \n",
    "as a graph of nodes with explicit **state**:\n",
    "\n",
    "- **StateGraph**: define a typed state (keys your nodes read/write).\n",
    "\n",
    "- **Nodes**: pure functions (sync or async) that transform state.\n",
    "\n",
    "- **Edges**: connect nodes; use **conditional edges** for **decision routing**.\n",
    "\n",
    "- **START / END**: special markers to begin and end execution.\n",
    "\n",
    "This explicit structure is ideal for BFSI where **auditable flows**, **guardrails**, and **determinism** matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d092368",
   "metadata": {},
   "source": [
    "\n",
    "## 2) LangGraph Basic Example â€” Typed State, Nodes, Edges\n",
    "\n",
    "We implement a simple **KYC guard â†’ Retriever â†’ Summarizer** flow.\n",
    "- If the query contains sensitive hints (e.g., `password`, `otp`, `pin`), we **block**.\n",
    "- Else we **retrieve** (stub) and **summarize** (stub).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "74b4970e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAFE PATH => {'query': 'Latest retail loan policy', 'route': 'process', 'docs': ['Policy doc snippet for: Latest retail loan policy', 'Interest rate depends on tenure and credit score; see policy 2024-09.'], 'summary': 'Summary: Policy doc snippet for: Latest retail loan policy | Interest rate depends on tenure and credit score; see policy 2024-09.'}\n",
      "BLOCK PATH => {'query': 'What is my OTP right now?', 'route': 'block', 'warning': 'ðŸš« Sensitive/PII-like term detected. Request denied.'}\n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "class BFState(TypedDict, total=False):\n",
    "    query: str\n",
    "    route: Literal[\"block\", \"process\"]\n",
    "    docs: list[str]\n",
    "    summary: str\n",
    "    warning: str\n",
    "\n",
    "RISK_WORDS = {\"password\", \"otp\", \"pin\", \"cvv\", \"secret\"}\n",
    "\n",
    "def kyc_guard(state: BFState) -> BFState:\n",
    "    q = state[\"query\"].lower()\n",
    "    if any(w in q for w in RISK_WORDS):\n",
    "        return {\"route\": \"block\", \"warning\": \"ðŸš« Sensitive/PII-like term detected. Request denied.\"}\n",
    "    return {\"route\": \"process\"}\n",
    "\n",
    "def retriever(state: BFState) -> BFState:\n",
    "    q = state[\"query\"]\n",
    "    # In production: query vector DB (Chroma/pgvector) or search service\n",
    "    return {\"docs\": [f\"Policy doc snippet for: {q}\",\n",
    "                     \"Interest rate depends on tenure and credit score; see policy 2024-09.\"]}\n",
    "\n",
    "def summarizer(state: BFState) -> BFState:\n",
    "    docs = state.get(\"docs\", [])\n",
    "    if not docs:\n",
    "        return {\"summary\": \"No data.\"}\n",
    "    return {\"summary\": \"Summary: \" + \" | \".join(docs)}\n",
    "\n",
    "graph = StateGraph(BFState)\n",
    "graph.add_node(\"kyc_guard\", kyc_guard)\n",
    "graph.add_node(\"retriever\", retriever)\n",
    "graph.add_node(\"summarizer\", summarizer)\n",
    "\n",
    "graph.add_edge(START, \"kyc_guard\")\n",
    "graph.add_conditional_edges(\n",
    "    \"kyc_guard\",\n",
    "    lambda s: s[\"route\"],\n",
    "    {\"block\": END, \"process\": \"retriever\"}\n",
    ")\n",
    "graph.add_edge(\"retriever\", \"summarizer\")\n",
    "graph.add_edge(\"summarizer\", END)\n",
    "\n",
    "app = graph.compile()\n",
    "\n",
    "print(\"SAFE PATH =>\", app.invoke({\"query\": \"Latest retail loan policy\"}))\n",
    "print(\"BLOCK PATH =>\", app.invoke({\"query\": \"What is my OTP right now?\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c0b813",
   "metadata": {},
   "source": [
    "\n",
    "### 3) LangGraph + GPT (via `langchain-openai`)\n",
    "\n",
    "We'll add an **LLM summarizer** node powered by OpenAI.  \n",
    "> Set `OPENAI_API_KEY` and choose a model via `OPENAI_MODEL`. If your workspace has **GPTâ€‘5**, set `OPENAI_MODEL=\"gpt-5\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b8a7e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM enabled: True\n",
      "{'query': 'Summarize retail loan policy changes', 'docs': ['Policy doc snippet for: Summarize retail loan policy changes', 'Risk weights revised in RBI circular X.'], 'llm_summary': '**Summary for Banking Analyst: Retail Loan Policy Changes and RBI Circular X**\\n\\n1. **Retail Loan Policy Changes:**\\n   - **Eligibility Criteria:** Adjustments have been made to the eligibility criteria for retail loans, potentially expanding the customer base by including more flexible income verification methods and considering alternative credit scoring models.\\n   - **Interest Rates:** There is a shift towards more competitive interest rates, with a focus on reducing rates for high-credit-score borrowers to attract low-risk customers.\\n   - **Loan-to-Value (LTV) Ratios:** The policy now allows for higher LTV ratios for certain categories of loans, such as home loans, to make borrowing more accessible.\\n   - **Processing Fees:** A reduction in processing fees has been implemented to encourage more applications and improve customer acquisition.\\n   - **Digital Lending:** Emphasis on enhancing digital lending platforms to streamline the application process and improve customer experience.\\n\\n2. **RBI Circular X - Revised Risk Weights:**\\n   - **Risk Weight Adjustments:** The Reserve Bank of India (RBI) has revised the risk weights for various asset classes, impacting the capital requirements for banks.\\n   - **Home Loans:** Risk weights for home loans have been adjusted based on the LTV ratio, with lower risk weights for loans with lower LTV ratios.\\n   - **Unsecured Loans:** Increased risk weights for unsecured personal loans to reflect higher default risk, necessitating higher capital reserves.\\n   - **Sector-Specific Changes:** Certain sectors, such as real estate and infrastructure, have seen changes in risk weights to align with current economic conditions and risk assessments.\\n   - **Implementation Timeline:** Banks are required to comply with the new risk weight guidelines within a specified timeframe to ensure a smooth transition and maintain regulatory compliance.\\n\\nThese changes aim to balance risk management with growth opportunities in the retail lending sector, while aligning with regulatory standards set by the RBI.'}\n"
     ]
    }
   ],
   "source": [
    "USE_LLM = bool(OPENAI_API_KEY)\n",
    "print(\"LLM enabled:\", USE_LLM)\n",
    "\n",
    "if USE_LLM:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.messages import HumanMessage\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-4o', temperature=0)\n",
    "\n",
    "    class LLMState(TypedDict, total=False):\n",
    "        query: str\n",
    "        docs: list[str]\n",
    "        llm_summary: str\n",
    "\n",
    "    async def llm_summarizer(state: LLMState) -> LLMState:\n",
    "        docs = state.get(\"docs\") or []\n",
    "        prompt_text = \"Summarize for a banking analyst:\\n\" + \"\\n\".join(f\"- {d}\" for d in docs) or \"No docs.\"\n",
    "        resp = await llm.ainvoke([HumanMessage(content=prompt_text)])\n",
    "        return {\"llm_summary\": resp.content}\n",
    "\n",
    "    from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "    g_llm = StateGraph(LLMState)\n",
    "    # reuse retriever from above for demo\n",
    "    def retriever_llm(state: LLMState) -> LLMState:\n",
    "        q = state[\"query\"]\n",
    "        return {\"docs\": [f\"Policy doc snippet for: {q}\", \"Risk weights revised in RBI circular X.\"]}\n",
    "\n",
    "    g_llm.add_node(\"retriever\", retriever_llm)\n",
    "    g_llm.add_node(\"llm_summarizer\", llm_summarizer)\n",
    "    g_llm.add_edge(START, \"retriever\")\n",
    "    g_llm.add_edge(\"retriever\", \"llm_summarizer\")\n",
    "    g_llm.add_edge(\"llm_summarizer\", END)\n",
    "\n",
    "    app_llm = g_llm.compile()\n",
    "\n",
    "\n",
    "    import nest_asyncio, asyncio\n",
    "    nest_asyncio.apply()\n",
    "    out = await app_llm.ainvoke({\"query\": \"Summarize retail loan policy changes\"})\n",
    "    print(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d4b5c7",
   "metadata": {},
   "source": [
    "Script/terminal version (if you move to a .py file)\n",
    "\n",
    "    Outside Jupyter (plain Python script), keep asyncio.run(...):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddf6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    out = asyncio.run(app_llm.ainvoke({\"query\": \"Summarize retail loan policy changes\"}))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be7f588",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Use Case: LangChain + LangGraph + GPT â€” Banking FAQ Summarizer\n",
    "\n",
    "Flow:\n",
    "1. Guard â†’ 2. Retrieve FAQs â†’ 3. LLM summarize â†’ 4. Return concise answer\n",
    "\n",
    "This pattern fits **contact center assistants**, **ops copilots**, or **policy desk bots**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3a5a0d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting faq_app_langgraph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile faq_app_langgraph.py\n",
    "\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5\")  # set to \"gpt-5\" if your account has access\n",
    "print(\"OPENAI_API_KEY set:\", bool(OPENAI_API_KEY))\n",
    "print(\"OPENAI_MODEL:\", OPENAI_MODEL)\n",
    "\n",
    "class FAQState(TypedDict, total=False):\n",
    "    question: str\n",
    "    route: Literal[\"block\", \"ok\"]\n",
    "    faqs: list[str]\n",
    "    answer: str\n",
    "    reason: str\n",
    "\n",
    "def guard(state: FAQState) -> FAQState:\n",
    "    q = state[\"question\"].lower()\n",
    "    if any(x in q for x in [\"password\", \"otp\", \"cvv\", \"pin\", \"secret\"]):\n",
    "        return {\"route\": \"block\", \"reason\": \"ðŸš« Sensitive query blocked.\"}\n",
    "    return {\"route\": \"ok\"}\n",
    "\n",
    "def retrieve_faqs(state: FAQState) -> FAQState:\n",
    "    q = state[\"question\"]\n",
    "    # Stub: In production, fetch from indexed policies / FAQ DB\n",
    "    return {\"faqs\": [f\"FAQ: Response policy related to: {q}\",\n",
    "                     \"Customers must not share OTP/CVV; see security policy 2025-A.\"]}\n",
    "\n",
    "if OPENAI_API_KEY:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    llm2 = ChatOpenAI(model=OPENAI_MODEL, temperature=0)\n",
    "\n",
    "    async def answer_llm(state: FAQState) -> FAQState:\n",
    "        faqs = state.get(\"faqs\") or []\n",
    "        prompt = \"Create a concise, compliant answer for a banking customer:\\n\" + \"\\n\".join(f\"- {f}\" for f in faqs)\n",
    "        resp = await llm2.ainvoke([HumanMessage(content=prompt)])\n",
    "        return {\"answer\": resp.content}\n",
    "else:\n",
    "    def answer_llm(state: FAQState) -> FAQState:\n",
    "        faqs = state.get(\"faqs\") or []\n",
    "        return {\"answer\": \"LLM disabled. Heuristic answer: \" + \" | \".join(faqs)}\n",
    "\n",
    "faq_graph = StateGraph(FAQState)\n",
    "faq_graph.add_node(\"guard\", guard)\n",
    "faq_graph.add_node(\"retrieve_faqs\", retrieve_faqs)\n",
    "faq_graph.add_node(\"answer_llm\", answer_llm)\n",
    "\n",
    "faq_graph.add_edge(START, \"guard\")\n",
    "faq_graph.add_conditional_edges(\n",
    "    \"guard\",\n",
    "    lambda s: s[\"route\"],\n",
    "    {\"block\": END, \"ok\": \"retrieve_faqs\"}\n",
    ")\n",
    "faq_graph.add_edge(\"retrieve_faqs\", \"answer_llm\")\n",
    "faq_graph.add_edge(\"answer_llm\", END)\n",
    "\n",
    "faq_app = faq_graph.compile()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import asyncio\n",
    "    out = asyncio.run(faq_app.ainvoke({\"question\": \"What is the current FD interest rate for 1 year?\"}))\n",
    "    #print(\"FAQ SAFE PATH =>\", out)\n",
    "    print(out['question'], \"=>\", out.get('answer'))\n",
    "\n",
    "    out1 = asyncio.run(faq_app.ainvoke({\"question\": \"Share my OTP please\"}))\n",
    "    print(\"FAQ BLOCK PATH =>\", out1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "68df3f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY set: True\n",
      "OPENAI_MODEL: gpt-5\n",
      "What is the current FD interest rate for 1 year? => FAQ: What is the current FD interest rate for 1 year?\n",
      "\n",
      "FD rates are dynamic and may change without notice. For the latest 1â€‘year FD rate applicable to your profile:\n",
      "- Visit our official website (Interest Rates > Fixed Deposits)\n",
      "- Check our mobile app/NetBanking (Open/Book FD to view live rates for your amount and tenor)\n",
      "- Visit your branch or call our official helpline\n",
      "\n",
      "Note: Rates can vary by customer category (e.g., senior citizen), deposit amount, tenor, and payout option.\n",
      "\n",
      "Security reminder (Policy 2025-A): Never share your OTP, CVV, card/PIN, or passwords. We will never ask for these over calls, emails, chat, or links. If unsure, contact us via official channels only.\n",
      "FAQ BLOCK PATH => {'question': 'Share my OTP please', 'route': 'block', 'reason': 'ðŸš« Sensitive query blocked.'}\n"
     ]
    }
   ],
   "source": [
    "!python faq_app_langgraph.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec74fb7",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Multiâ€‘Agent Orchestration â€” Coordinating Agents, Context Passing, Decision Routing\n",
    "\n",
    "We build a small **router** that sends queries to one of two agents:\n",
    "- **Calc Agent**: safely evaluates basic arithmetic (no `eval`).\n",
    "- **Policy Agent**: returns policy-search stubs (replace with vector DB later).\n",
    "\n",
    "**Context Passing**: both agents read from the same typed state; router sets a `route` key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "35937258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum 18 + 24 => 42\n",
      "policy for retail loans under 25L? => ðŸ”Ž (Stub) Policy search result for: policy for retail loans under 25L?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ast, operator as op\n",
    "from typing import TypedDict, Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "OPS = {ast.Add: op.add, ast.Sub: op.sub, ast.Mult: op.mul, ast.Div: op.truediv}\n",
    "\n",
    "class RouteState(TypedDict, total=False):\n",
    "    query: str\n",
    "    route: Literal[\"calc_agent\", \"policy_agent\"]\n",
    "    result: str\n",
    "\n",
    "def router(state: RouteState) -> RouteState:\n",
    "    q = state[\"query\"].lower()\n",
    "    if any(tok in q for tok in [\"+\", \"-\", \"*\", \"/\", \"sum\", \"calculate\"]):\n",
    "        return {\"route\": \"calc_agent\"}\n",
    "    return {\"route\": \"policy_agent\"}\n",
    "\n",
    "def calc_agent(state: RouteState) -> RouteState:\n",
    "    text = state[\"query\"].lower().replace(\"sum\", \"\").replace(\"calculate\", \"\").strip()\n",
    "    try:\n",
    "        node = ast.parse(text, mode=\"eval\").body\n",
    "        def _eval(n):\n",
    "            if isinstance(n, ast.Num):  # type: ignore[attr-defined]\n",
    "                return float(n.n)\n",
    "            if isinstance(n, ast.BinOp) and type(n.op) in OPS:\n",
    "                return OPS[type(n.op)](_eval(n.left), _eval(n.right))\n",
    "            raise ValueError(\"Only + - * / supported.\")\n",
    "        val = _eval(node)\n",
    "        return {\"result\": f\"{val:g}\"}\n",
    "    except Exception:\n",
    "        return {\"result\": \"Failed to parse expression.\"}\n",
    "\n",
    "def policy_agent(state: RouteState) -> RouteState:\n",
    "    q = state[\"query\"]\n",
    "    return {\"result\": f\"ðŸ”Ž (Stub) Policy search result for: {q}\"}\n",
    "\n",
    "rg = StateGraph(RouteState)\n",
    "rg.add_node(\"router\", router)\n",
    "rg.add_node(\"calc_agent\", calc_agent)\n",
    "rg.add_node(\"policy_agent\", policy_agent)\n",
    "\n",
    "rg.add_edge(START, \"router\")\n",
    "rg.add_conditional_edges(\n",
    "    \"router\",\n",
    "    lambda s: s[\"route\"],\n",
    "    {\"calc_agent\": \"calc_agent\", \"policy_agent\": \"policy_agent\"}\n",
    ")\n",
    "rg.add_edge(\"calc_agent\", END)\n",
    "rg.add_edge(\"policy_agent\", END)\n",
    "\n",
    "route_app = rg.compile()\n",
    "\n",
    "import asyncio\n",
    "\n",
    "out= asyncio.run(route_app.ainvoke({\"query\": \"sum 18 + 24\"}))\n",
    "print(out[\"query\"], \"=>\", out[\"result\"])\n",
    "\n",
    "out1 = asyncio.run(route_app.ainvoke({\"query\": \"policy for retail loans under 25L?\"}))\n",
    "#print(out1)\n",
    "print(out1[\"query\"], \"=>\", out1[\"result\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
